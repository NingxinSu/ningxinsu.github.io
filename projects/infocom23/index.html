<!doctype html><html lang=en><head><title>Asynchronous Federated Unlearning ::
Ningxin Su
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Ningxin Su, Baochun Li, University of Toronto, IEEE International Conference on Computer Communications (INFOCOM) 2023 [Paper] [Slides] [Source Code]
Thanks to regulatory policies such as the General Data Protection Regulation (GDPR), it is essential to provide users with the right to erasure regarding their own private data, even if such data has been used to train a neural network model. Such a machine unlearning problem becomes even more challenging in the context of federated learning. where clients collaborate to train a global model with their private data. When a client requests that its data be erased, its effects have already gradually permeated through a large number of clients, as the server aggregates client updates over multiple communication rounds. Thus, erasing data samples from one client requires a large number of clients to engage in a retraining process.
"><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=/projects/infocom23/><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/style.css><link rel=apple-touch-icon-precomposed sizes=144x144 href=/img/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=/img/favicon.png><link href=/fonts/Inter-Italic.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=/fonts/Inter-Regular.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=/fonts/Inter-Medium.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=/fonts/Inter-MediumItalic.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=/fonts/Inter-Bold.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=/fonts/Inter-BoldItalic.woff2 rel=preload type=font/woff2 as=font crossorigin><meta name=twitter:card content="summary"><meta name=twitter:title content="Asynchronous Federated Unlearning"><meta name=twitter:description content="Ningxin Su, Baochun Li, University of Toronto, IEEE International Conference on Computer Communications (INFOCOM) 2023 [Paper] [Slides] [Source Code]
Thanks to regulatory policies such as the General Data Protection Regulation (GDPR), it is essential to provide users with the right to erasure regarding their own private data, even if such data has been used to train a neural network model. Such a machine unlearning problem becomes even more challenging in the context of federated learning. where clients collaborate to train a global model with their private data. When a client requests that its data be erased, its effects have already gradually permeated through a large number of clients, as the server aggregates client updates over multiple communication rounds. Thus, erasing data samples from one client requires a large number of clients to engage in a retraining process."><meta property="og:url" content="/projects/infocom23/"><meta property="og:site_name" content="Ningxin Su"><meta property="og:title" content="Asynchronous Federated Unlearning"><meta property="og:description" content="Ningxin Su, Baochun Li, University of Toronto, IEEE International Conference on Computer Communications (INFOCOM) 2023 [Paper] [Slides] [Source Code]
Thanks to regulatory policies such as the General Data Protection Regulation (GDPR), it is essential to provide users with the right to erasure regarding their own private data, even if such data has been used to train a neural network model. Such a machine unlearning problem becomes even more challenging in the context of federated learning. where clients collaborate to train a global model with their private data. When a client requests that its data be erased, its effects have already gradually permeated through a large number of clients, as the server aggregates client updates over multiple communication rounds. Thus, erasing data samples from one client requires a large number of clients to engage in a retraining process."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="projects"></head><body class=light-theme><div class=container><header class=header><span class=header__inner><a href=/ class=logo style=text-decoration:none><span class=logo__mark><svg class="greater-icon" viewBox="0 0 44 44"><path fill="none" d="M15 8l14.729 14.382L15 35.367"/></svg>
</span><span class=logo__text>Ningxin Su</span>
<span class=logo__cursor></span>
</a><span class=header__right><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/publications>Publications</a></li><li><a href=/awards>Awards</a></li><li><a href=/teaching>Teaching</a></li><li><a href=/professional_services>Professional Services</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/publications>Publications</a></li><li><a href=/awards>Awards</a></li><li><a href=/teaching>Teaching</a></li><li><a href=/professional_services>Professional Services</a></li></ul></nav><span class=menu-trigger><svg viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg>
</span><span class=theme-toggle><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none"><path d="M22 41c10.4934.0 19-8.5066 19-19C41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><article class=post><h1 class=post-title>Asynchronous Federated Unlearning</h1><div class=post-meta></div><div class=post-content><p><strong>Ningxin Su</strong>, Baochun Li, University of Toronto,<br><em>IEEE International Conference on Computer Communications (INFOCOM) 2023</em><br>[<a href=/assets/infocom23.pdf>Paper</a>] [<a href=/assets/infocom23_slides.pdf>Slides</a>] [<a href=https://github.com/TL-System/plato/tree/main/examples/knot>Source Code</a>]</p><p>Thanks to regulatory policies such as the General Data Protection Regulation (GDPR), it is essential to provide users with the <em>right to erasure</em> regarding their own private data, even if such data has been used to train a neural network model. Such a <em>machine unlearning</em> problem becomes even more challenging in the context of federated learning. where clients collaborate to train a global model with their private data. When a client requests that its data be erased, its effects have already gradually permeated through a large number of clients, as the server aggregates client updates over multiple communication rounds. Thus, erasing data samples from one client requires a large number of clients to engage in a retraining process.</p><p>In asynchronous FL, different clients progress at different speeds naturally in their local training. Intuitively, as Figure 1 shows, if we allow some clients to move forward towards global convergence while retraining only a small subset of the clients as data samples are erased, the inherent overhead of retraining can be substantially mitigated. A simple yet effective mechanism is to divide all clients into a small number of clusters, and aggregate client updates within the confines of each cluster only. The immediate effect of such clustered aggregation is that, if any client requests that its data samples be erased, only clients within the same cluster need to be retrained, while other unaffected clusters may continue with normal FL training.</p><img src=/assets/infocom23-asynch.png alt="In asynchronous FL, clients spend different amounts of time for training and communication, and the server only needs to wait for a subset of clients to report back." class=center style=border-radius:8px;width:60%><p><em>Figure 1: In asynchronous FL, clients spend different amounts of time for training and communication, and the server only needs to wait for a subset of clients to report back.</em></p><h3 id=machine-unlearning>Machine Unlearning
<a href=#machine-unlearning class=h-anchor aria-hidden=true>#</a></h3><p>With the recent emergence of regulatory policies that require corporations to provide the right to erase private data from users, machine unlearning has garnered increasing research attention. As illustrated in Figure 2, with the machine unlearning process, a new neural network model should be produced by unlearning all the effects of any private data that is requested to be erased. Existing research on machine unlearning aims to produce the unlearned model with the highest possible efficiency, without taking the naïve approach of retraining an initial model from scratch.</p><img src=/assets/infocom23-machineul.png alt="The primary goal of the machine unlearning process is to produce a new neural network model with the erased data unlearned, equivalent to a model initialized and then trained without using the erased data." class=center style=border-radius:8px;width:72%><p><em>Figure 2: The primary goal of the machine unlearning process is to produce a new neural network model with the erased data unlearned, equivalent to a model initialized and then trained without using the erased data.</em></p><h3 id=_knot_-mdash-our-new-federated-unlearning-paradigm><em>Knot</em> — Our New Federated Unlearning Paradigm
<a href=#_knot_-mdash-our-new-federated-unlearning-paradigm class=h-anchor aria-hidden=true>#</a></h3><p>We propose to construct a &ldquo;firewall&rdquo; between <em>clusters</em> of clients in the system, such that server aggregation is carried out within each cluster only. With such a mechanism, which we refer to as <em>clustered aggregation</em>, the ripple effect of one client&rsquo;s contributions will only affect other clients in the same cluster, and clients outside the cluster will no longer need to be retrained. Figure 3 shows how clients can be clustered and how the server aggregates the updates within each cluster only. It is worth noting that our new mechanism of clustered aggregation is orthogonal to any innovations in the retraining mechanism: it is complementary to both the naïve mechanism of retraining from scratch and all its alternatives using approximation algorithms.</p><img src=/assets/infocom23-fedul.png alt="An example of asynchronous clustered aggregation where four clients have been assigned to two clusters. If client \#4 requests to erase the effects of its data from the server, only clients in cluster \#2 need to be retrained, while clients in cluster \#1 may proceed normally with their FL training process." class=center style=border-radius:8px;width:80%><p><em>Figure 3: Example of asynchronous clustered aggregation where four clients have been assigned to two clusters. If client #4 requests to erase the effects of its data from the server, only clients in cluster #2 need to be retrained, while clients in cluster #1 may proceed normally with their FL training process.</em></p><p>While existing work on federated unlearning focused on improving its performance with more efficient retraining algorithms, we sought to take a decidedly different and orthogonal approach in this work with the introduction of clustered aggregation. The efficacy of our approach hinges on the intuition that the retraining process can be constrained to within a cluster only, if server aggregation is only performed within each cluster, and training sessions in the other clusters can be carried out asynchronously. We are the first to propose <em>asynchronous federated unlearning</em>, taking advantage of the well-recognized performance benefit of asynchronous FL. <em>Knot</em>, our optimization-based clustered aggregation mechanism, pushes the performance envelope further by not only formulating the client-cluster assignment problem as a lexicographical minimization problem, but also proving that it can be solved efficiently using a linear program solver. With our scalable and reproducible implementation, we have shown in Table 1 that the wall-clock training time with <em>Knot</em> is up to 85% better than FedEraser, a state-of-the-art approximation algorithm, even when retraining from scratch. Last but not least, our experiments were designed to be reproducible, and we have provided public access to <a href=https://github.com/TL-System/plato/tree/main/examples/knot>our source code</a>.</p><img src=/assets/infocom23-results.png alt=Results. class=center style=border-radius:8px;width:70%><p><em>Table 1: Knot vs. its competitors: a comparison regarding the wall-clock times needed to converge to a target accuracy (or perplexity).</em></p><h2 id=bibtex>BibTeX
<a href=#bibtex class=h-anchor aria-hidden=true>#</a></h2><pre tabindex=0><code>@inproceedings{knot_su2023,    
   author = {Ningxin Su and Baochun Li},      
   journal = {Proc.~Int&#39;l Conference on Computer Communications (INFOCOM)},     
   publisher = {IEEE},                 
   title = {Asynchronous Federated Unlearning},                
   year = {2023},        
}
</code></pre></div></article></div><footer class=footer><div class=footer__inner><div class="copyright copyright--user">Ningxin Su&copy;2025</div></div></footer><script type=text/javascript src=/bundle.min.js></script></div></body></html>