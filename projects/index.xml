<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Projects on Ningxin Su</title><link>/projects/</link><description>Recent content in Projects on Ningxin Su</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Ningxin Su&amp;copy;</copyright><atom:link href="/projects/index.xml" rel="self" type="application/rss+xml"/><item><title>Asynchronous Federated Unlearning</title><link>/projects/infocom23/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/projects/infocom23/</guid><description>
&lt;p>&lt;strong>Ningxin Su&lt;/strong>, Baochun Li, University of Toronto, &lt;br>
&lt;em>IEEE International Conference on Computer Communications (INFOCOM) 2023&lt;/em> &lt;br>
[&lt;a href="/assets/infocom23.pdf">Paper&lt;/a>] [&lt;a href="/assets/infocom23_slides.pdf">Slides&lt;/a>] [&lt;a href="https://github.com/TL-System/plato/tree/main/examples/knot">Source Code&lt;/a>]&lt;/p>
&lt;p>Thanks to regulatory policies such as the General Data Protection Regulation (GDPR), it is essential to provide users with the &lt;em>right to erasure&lt;/em> regarding their own private data, even if such data has been used to train a neural network model. Such a &lt;em>machine unlearning&lt;/em> problem becomes even more challenging in the context of federated learning. where clients collaborate to train a global model with their private data. When a client requests that its data be erased, its effects have already gradually permeated through a large number of clients, as the server aggregates client updates over multiple communication rounds. Thus, erasing data samples from one client requires a large number of clients to engage in a retraining process.&lt;/p></description></item><item><title>How Asynchronous can Federated Learning Be?</title><link>/projects/iwqos22/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/projects/iwqos22/</guid><description>
&lt;p>&lt;strong>Ningxin Su&lt;/strong>, Baochun Li, University of Toronto, &lt;br>
&lt;em>IEEE International Symposium on Quality of Service (IWQoS) 2022&lt;/em> &lt;br>
[&lt;a href="/assets/iwqos22.pdf">Paper&lt;/a>] [&lt;a href="/assets/port_slides.pdf">Slides&lt;/a>] [&lt;a href="https://github.com/TL-System/plato/tree/main/examples/port">Source Code&lt;/a>]&lt;/p>
&lt;p>As a practical paradigm designed to involve large numbers of edge devices in distributed training of deep learning models, federated learning has witnessed a significant amount of research attention in the recent years. Yet, most existing mechanisms on federated learning assumed either fully synchronous or asynchronous communication strategies between clients and the federated learning server. Existing designs that were partially asynchronous in their communication were simple heuristics, and were evaluated using the number of communication rounds or updates required for convergence, rather than the wall-clock time in practice.&lt;/p></description></item></channel></rss>